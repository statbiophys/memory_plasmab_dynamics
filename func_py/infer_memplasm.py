import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from scipy.stats import poisson, expon, gamma, uniform, nbinom
from itertools import product
import sys
import my_utils as ut
import data_utils as dt
sys.path.insert(0, "../func_build/")
import like_func as lf
from multiprocessing import Pool, cpu_count
from tqdm.notebook import tqdm



class mp_infer_pars():
    """
    Parameters to pass to the likelihood MC computation
    """
    
    def __init__(self, dtimes, sample_indexes, tau_m, theta_m, Mm_tot, n0, m_der_f, p_to_m, n_eval, prior_pars, dts, 
                 n1_min=0, a_negbin=[], n_discr=2000):
        
        # delta times between time steps
        self.dtimes = dtimes
        # list of which time indexes are present for memory and plasmablasts
        self.m_indexes = sample_indexes[0]
        self.p_indexes = sample_indexes[1]
        # Tau of memory cells
        self.tau_m = tau_m
        # Theta of memory cells
        self.theta_m = theta_m
        # Total number of cells
        self.Mm_tot = Mm_tot
        # Number of cells at birth
        self.n0 = n0
        # Fraction of plasmablasts generated by memory cells
        self.m_der_f = m_der_f
        # Total plasmablasts - total memory cells fraction
        self.p_to_m = p_to_m
        # Minimum number of exp counts at first time to condition the inference
        self.n1_min = n1_min
        # Number of MC samples to evaluate the LL
        self.n_eval = n_eval
        # Priors on the gamma function parameters for the importance sampling
        self.prior_pars = prior_pars
        # a parameters of the neg-bin noise for each time step. If empty list we consider a Poisson
        # first element list of as for memory samples, second list for plasmablasts
        self.a_negbin = a_negbin
        # Time step for the trajectory generation
        self.dts = dts
        # Number of bins of the log count space to compute the integral
        self.n_discr = n_discr

        
        
def import_data(mem_samples, pb_samples, fam_type='familiy_pairs', count_label='pair_count'):
    """
    Import the samples and collapse them in a sparse frame from a list of names of memory and plasmablast
    samples. The number of elements in the two list must be the same, and an empty string is used to indicate
    that the sample is not present at the given time point.
    """
    
    def get_counts(name, short_id):
        sample = dt.read_family_frame(name, fam_type=fam_type)
        if count_label not in sample.columns:
            print('count label not found, setting the counts to 1')
            sample[count_label] = 1
        clone_counts = sample.groupby(fam_type).agg({count_label : sum})
        return clone_counts.rename({count_label : 'counts_' + short_id}, axis=1)
        
    pars_d = {'m_ids' : [], 'p_ids' : []}
    aux = pd.DataFrame()
    for t, name in enumerate(mem_samples):
        if name == '': continue
        pars_d['m_ids'].append(t)
        clone_counts = get_counts(name, 'm' + str(t+1))
        aux = pd.merge(aux, clone_counts, how='outer', left_index=True, right_index=True).fillna(0)
        
    if pars_d['m_ids'][0] != 0:
        print('first sample of memory cells not present!')
        
    for t, name in enumerate(pb_samples):
        if name == '': continue
        pars_d['p_ids'].append(t)
        clone_counts = get_counts(name, 'p' + str(t+1))
        aux = pd.merge(aux, clone_counts, how='outer', left_index=True, right_index=True).fillna(0)        
        
    # Sparsifying
    aux = aux.astype(int).astype(str)
    aux['count_tuple'] = ''
    agg_d = dict()
    for t, name in enumerate(mem_samples):
        if name == '': continue
        aux['count_tuple'] += aux['counts_m' + str(t+1)] + "_"
        agg_d['counts_m' + str(t+1)] = 'first'
    for t, name in enumerate(pb_samples):
        if name == '': continue
        aux['count_tuple'] += aux['counts_p' + str(t+1)]
        if name != pb_samples[-1]: aux['count_tuple'] += '_'
        agg_d['counts_p' + str(t+1)] = 'first'
        
    aux['occ'] = 1
    agg_d['occ'] = sum
    sp_counts = aux.groupby('count_tuple').agg(agg_d)
    sp_counts = sp_counts.astype(int).sort_values('occ', ascending=False)
    
    return sp_counts, pars_d
        
def sample(tau_p, theta_p, pars, import_f):
    # rho obtained from equating the number of pb as Mmem fraction to the stationary M of pb
    rho = pars.p_to_m * pars.m_der_f / tau_p 
    # Sampling from importance function
    x1_samp = import_f.rvs(pars.n_eval)
    # Parameters for the traj generator. n0=0 imposes no clone creation
    n0_eff = 0
    if pars.n1_min <= 0: n0_eff = pars.n0 # Creating new clones only if we do not constraint on m1
    mp_pars = lf.memplasm_pars(pars.tau_m, pars.theta_m, rho, tau_p, theta_p, n0_eff) 
    # Obraining the log-count samples at the desired time points from traj generator
    return gen_memplasm_multitraj(mp_pars, pars.dtimes, pars.dts, x1_samp, pars.n_discr)
    
    
def log_like_MC(sp_counts, tau_p, theta_p, pars):   
    
    # Computing experimental counts from sp_counts dataframe
    Mms = [np.sum(sp_counts['counts_m'+str(i+1)]*sp_counts['occ']) for i in pars.m_indexes]
    Mps = [np.sum(sp_counts['counts_p'+str(i+1)]*sp_counts['occ']) for i in pars.p_indexes]
    
    # Defining the importance function
    n_max = max(sp_counts.counts_m1.values)
    import_f = get_gamma_prior(n_max, Mms[0], pars.Mm_tot, pars.prior_pars[0], pars.prior_pars[1])
    
    # Setting the noise model
    if len(pars.a_negbin) == 0: # Poisson case
        a_nb_mem, a_nb_pb = np.zeros(len(Mms)), np.zeros(len(Mps))
        noise_f = lambda mu, a, n : poisson(mu).pmf(n)
    else: # Negative binomial with b=1 case
        a_nb_mem, a_nb_pb = np.copy(pars.a_negbin[0]), np.copy(pars.a_negbin[1])
        noise_f = lambda mu, a, n : neg_bin_f(mu, a).pmf(n)
        
    # Generation of the samples
    xs, ys = sample(tau_p, theta_p, pars, import_f)

    # Discretizing the values in bins to speed up the computation of integrand
    xs_bins, ys_bins, x_vals, y_vals = discretize_logcounts(xs, ys, pars.n_discr)

    # Evaluating the integrands at the central values of each bin
    # A zero is appended in front, which corresponds to the bin at count = 0 (logcount = -1)
    auxp1 = np.append(0, stat_dist(x_vals, pars) / import_f.pdf(x_vals))
    m_vals = np.exp(x_vals)
    m_mus, p_mus = [], [] # Averages of the noise model for memory and plasmablasts
    for i in range(len(pars.m_indexes)): 
        m_mus.append( np.append(0, m_vals * Mms[i] / pars.Mm_tot) )
    Mp_cells = pars.Mm_tot * pars.p_to_m
    p_vals = np.exp(y_vals)
    for i in range(len(pars.p_indexes)): 
        p_mus.append( np.append(0, p_vals * Mps[i] / Mp_cells) ) # Freq = p_count / total pb (also the non-mem derived)

    # Computing the likelihood by summing each configuration of counts
    fr = sp_counts.copy()
    tot_samples, ll = 0, 0
    for _id, row in sp_counts.iterrows():
        
        if row.counts_m1 < pars.n1_min: continue
        nm_sum = sum([row['counts_m'+str(mi+1)] for mi in pars.m_indexes])
        if nm_sum == 0: continue
            
        N_old = int(pars.n_eval)
        # Computation for clones generated at start for which I can do importance sampling
        ps = (auxp1 * noise_f(m_mus[0], a_nb_mem[0], row['counts_m1']))[xs_bins[0,:N_old]]
        for i, mi in enumerate(pars.m_indexes[1:]):
            ps *= noise_f(m_mus[i+1], a_nb_mem[i+1], row['counts_m'+str(mi+1)])[xs_bins[mi,:N_old]]
        for i, pi in enumerate(pars.p_indexes):
            ps *= noise_f(p_mus[i], a_nb_pb[i], row['counts_p'+str(pi+1)])[ys_bins[pi,:N_old]]
        weight_at_existing = np.sum(ps)
    
        if row.counts_m1 == 0: # Computation for new clones (it happens only if n1_min=0)
            ps = np.ones(len(xs_bins[0]) - N_old)
            for i, mi in enumerate(pars.m_indexes[1:]):
                ps *= noise_f(m_mus[i+1], a_nb_mem[i+1], row['counts_m'+str(mi+1)])[xs_bins[mi,N_old:]]
            for i, pi in enumerate(pars.p_indexes):
                ps *= noise_f(p_mus[i], a_nb_pb[i], row['counts_p'+str(pi+1)])[ys_bins[pi,N_old:]]
            weight_at_new = np.sum(ps)
        else:
            weight_at_new = 0
        
        p = (weight_at_new + weight_at_existing) / len(xs_bins[0])
        if p > 0:
            ll += np.log(p) * sp_counts.loc[_id, 'occ']
            tot_samples += sp_counts.loc[_id, 'occ']
            fr.loc[_id, 'll_term'] = p
            
    # p is not yet a real probability because it has to be conditioned to the prob of non observing 
    # any mem sample, which however does not depend on the plasmablast dynamics
    if pars.n1_min > 0:
        p10 = compute_p_smaller_mmin(pars, Mms[0], pars.n_eval*10)
        ll = (ll - tot_samples * np.log(max(1e-300, 1 - p10)))
        
    return ll / tot_samples , fr

def compute_sp_count_probs(sp_counts, tau_p, theta_p, mp_pars, max_m, max_p):
    """
    It fills the sp_counts with all the combinations of counts smaller than max_m for memory and max_p for 
    plasmablasts.
    """
    md, pd = len(mp_pars.m_indexes), len(mp_pars.p_indexes)
    prod_set = [np.arange(max_m)]*md
    for _ in range(pd):
        prod_set.append( np.arange(max_p) )

    for ns in product(*prod_set):
        _id = ""
        for i, n in enumerate(ns):
            _id += str(n) + "_"
        _id = _id[:-1]

        if _id not in sp_counts.index:
            for i, si in enumerate(mp_pars.m_indexes):
                sp_counts.loc[_id, 'counts_m'+str(si+1)] = ns[i]
            for i, si in enumerate(mp_pars.p_indexes):
                sp_counts.loc[_id, 'counts_p'+str(si+1)] = ns[i+len(mp_pars.m_indexes)]
                
    return log_like_MC(sp_counts, tau_p, theta_p, mp_pars)[1]


def neg_bin_f(mu, a_negbin):
    """
    Negative binomial of the noise model with our parametrization and b = 1
    """
    mu[mu == 0] = 1e-10
    s2 = mu * (a_negbin + 1)
    return nbinom(mu * mu / (s2 - mu), mu / s2)


def stat_dist(x, pars):
    """
    Stat distribution of the geometric brownian motion given a dictionary parameters
    """
    x0 = np.log(pars.n0)
    a = 2 * pars.theta_m / pars.tau_m
    coef = 1 / x0
    return np.where(x < x0, coef * (1 - np.exp(-a*x)), coef * (np.exp(a*(x0-x)) - np.exp(-a*x)))


def get_gamma_prior(n_max, M1, M_tot, gamma_a, std_factor=3):
    """
    Gamma distribution prior
    """
    x_max_eff = np.log(2*n_max*M_tot/M1)
    gamma_std = x_max_eff / std_factor
    return gamma(gamma_a, 0, scale=gamma_std/np.sqrt(gamma_a))


def _gen_memplasm_traj_parall(args):
    """
    Aux function that is run in parallel by gen_memplasm_traj
    """
    tau_m, theta_m, rho, tau_p, theta_p, n0, delta_t, x0s, n0s, dt, i_chunk, seed = args
    pars = lf.memplasm_pars(tau_m, theta_m, rho, tau_p, theta_p, n0)
    ns = lf.gen_memplasm_traj(pars, x0s, n0s, delta_t, dt, seed + i_chunk) # using n_chunk as seed
    return ns, i_chunk


def gen_memplasm_traj(pars, delta_t, dt, x0s, n0s=[], chunk_size=10000, n_threads=4, seed=0):
    """
    Generation of memory and plasmablast trajectories parallelizing for chunks of given sizes.
    If the initial condition for plasmablast counts is n0s=[], the init cond will be the stationary
    distribution.
    If the argument n0 of pars is zero no new clones are generated.
    """
    arg_list = []
    i = 0
    while i < len(x0s):
        n0s_chunck = []
        if len(n0s) == len(x0s):
            n0s_chunck = n0s[i:i+chunk_size]
        arg_list.append((pars.tau_m, pars.theta_m, pars.rho, pars.tau_p, pars.theta_p, pars.n0,
                         delta_t, x0s[i:i+chunk_size], n0s_chunck, dt, i, seed))
        i += chunk_size
    result = ut.parallelize_async(_gen_memplasm_traj_parall, arg_list, n_threads)
    
    # Parsing the output of each chunk
    ns_chunks, i_chunks = [], []
    for ns_chunk, i_chunk in result:
        ns_chunks.append(ns_chunk)
        i_chunks.append(i_chunk)
        
    xs, ns = np.array([]), np.array([])
    for i in np.argsort(i_chunks):
        xs = np.append(xs, ns_chunks[i][0])
        ns = np.append(ns, ns_chunks[i][1])
    return xs, ns


def gen_memplasm_multitraj(pars, delta_ts, dts, x0s, chunk_size=2000, seed=0):
    """
    Generation of different time points of the trajectories given initial condition of the 
    memory log counts 
    """
    ns0 = np.exp(x0s) * pars.rho * pars.tau_p
    xs, ys, ns = [x0s], [np.log(ns0)], [ns0]
    for i in range(len(delta_ts)):
        time, dt = delta_ts[i], dts[i]
        #x, n = mpg.gen_memplasm_traj_parall(pars, time, dt, xs[-1], ns[-1], n_threads=n_threads, seed=seed)
        #mp_pars = lf.memplasm_pars(pars.tau_m, pars.theta_m, pars.rho, pars.tau_p, pars.theta_p, pars.n0)
        x, n = lf.gen_memplasm_traj(pars, xs[-1], ns[-1], time, dt, seed)
        #x, n = gen_memplasm_traj(pars, time, dt, xs[-1], ns[-1], chunk_size, n_threads, seed)
        xs.append(np.array(x))
        ns.append(np.array(n))
        zero_mask = ns[-1] >= 1
        ys.append(np.where(zero_mask, np.log(n, where=zero_mask), -1))
    
    n_clones = len(xs[-1])
    for i in range(len(delta_ts)):
        if len(xs[i]) < n_clones:
            xs[i] = np.append(xs[i], -np.ones(n_clones - len(xs[i])))
        if len(ys[i]) < n_clones:
            ys[i] = np.append(ys[i], -np.ones(n_clones - len(ys[i])))
            
    return np.array(xs), np.array(ys)


def compute_p_smaller_mmin(pars, M1, n_eval):
    """
    Computing prob of P smaller than a m_min at first time point
    """
    points = np.linspace(0, np.log(pars.Mm_tot), n_eval)
    dx = points[1] - points[0]
    mu1s = np.exp(points) * M1 / pars.Mm_tot
    aux = stat_dist(points, pars)
    ps = 0
    for m in range(pars.n1_min):
        ps += np.sum(aux * poisson(mu1s).pmf(m)) * dx
    return ps


def discretize_logcounts(xs, ys, n_discr=2000):
    """
    Discretization of log counts
    """
    x_bins = np.linspace(0, np.max(xs), n_discr)
    x_vals = x_bins + (x_bins[1] - x_bins[0]) / 2.0
    y_bins = np.linspace(0, np.max(ys), n_discr)
    y_vals = y_bins + (y_bins[1] - y_bins[0]) / 2.0
    
    # List of bin indexes for each logcount. logcount = -1 are put in bin 0
    xs_bins = np.array([np.digitize(x, x_bins) for x in xs])
    ys_bins = np.array([np.digitize(y, y_bins) for y in ys])
    
    return xs_bins, ys_bins, x_vals, y_vals


def plot_heatmap(ax, mat, xs, ys, vmin, vmax, labx='theta', laby='tau', cb=True):
    ax.set_xlabel(labx, fontsize=12)
    ax.set_ylabel(laby, fontsize=12)
    ax.set_xticks(np.arange(0, len(xs)))
    ax.set_xticklabels(["%2.1f"%s for s in xs])
    ax.set_yticks(np.arange(0, len(ys)))
    ax.set_yticklabels(["%2.1f"%s for s in ys[::-1]])
    im = ax.imshow(mat[::-1], vmin=vmin, vmax=vmax)
    if cb:
        cb = plt.colorbar(im, ax=ax)
    return ax, cb
