{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcb7898a-60e8-4aac-8c4d-1dd34802186d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.insert(0, \"../func_py/\")\n",
    "import data_utils as ut\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool, cpu_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525a577f-180e-4882-85cc-3744aa669a97",
   "metadata": {},
   "source": [
    "### Here we format the input file for the hilary software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bd0ab99-fd85-4c2e-b1ca-d40a6e56f607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyParallel(frame, func, silent=False, threads=None):\n",
    "    \"\"\"\n",
    "    Parallel computing across rows of a Dataframe\n",
    "    \"\"\"\n",
    "    max_threads = cpu_count()\n",
    "    if threads is None: \n",
    "        threads = max_threads\n",
    "    elif threads > max_threads: \n",
    "        threads = max_threads\n",
    "    \n",
    "    _dfGrouped = frame.groupby(level=0)\n",
    "    with Pool(threads) as p:\n",
    "        ret_list = list(tqdm(p.imap(func, _dfGrouped), total=len(_dfGrouped), disable=silent))\n",
    "        \n",
    "    return pd.concat(ret_list)\n",
    "\n",
    "\n",
    "def hilary_parse(args):\n",
    "    \"\"\"\n",
    "    Parsing a single row of a frame for Hilary formatting\n",
    "    \"\"\"\n",
    "    row = args[1].loc[args[0]]\n",
    "    d = dict()\n",
    "    try:\n",
    "        v_st, v_end = int(row.v_germline_start), int(row.v_germline_end)\n",
    "        j_len = int(row.j_sequence_end - row.j_sequence_start)\n",
    "        j_st = row.germline_alignment.rfind('N')+1\n",
    "        d['v_sequence_alignment'] = row.sequence_alignment[v_st-1 : v_end]\n",
    "        d['j_sequence_alignment'] = row.sequence_alignment[j_st : j_st + j_len]\n",
    "        d['v_germline_alignment'] = row.germline_alignment[v_st-1 : v_end]\n",
    "        d['j_germline_alignment'] = row.germline_alignment[j_st : j_st + j_len]\n",
    "        return pd.DataFrame(d, index=[args[0]])\n",
    "    except:\n",
    "        print(args[0], row)\n",
    "        return pd.DataFrame(index=[args[0]])\n",
    "\n",
    "    \n",
    "def hilary_preprocess(f):\n",
    "    \"\"\"\n",
    "    Formatting the data as hilary wants (parallel computation)\n",
    "    \"\"\"\n",
    "    df = applyParallel(f, hilary_parse)\n",
    "    df['junction'] = df.index.map(f.junction)\n",
    "    df['v_call'] = df.index.map(f.v_call)\n",
    "    df['j_call'] = df.index.map(f.j_call)\n",
    "    df['count'] = df.index.map(f.pair_count)\n",
    "    df.index.name = 'sequence_id'\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_agg_dict(f):\n",
    "    \"\"\"\n",
    "    Dictionary used for the sequence collapsing\n",
    "    \"\"\"\n",
    "    agg_dict = { k:'first' for k in f.columns }\n",
    "    agg_dict['pair_count'] = sum\n",
    "    agg_dict['seq_ids_list'] = list\n",
    "    return agg_dict\n",
    "\n",
    "\n",
    "def collapse_heavy(f):\n",
    "    \"\"\"\n",
    "    Collapsing for identical heavy sequence\n",
    "    \"\"\"\n",
    "    f = f[f.germline_alignment.notna()]\n",
    "    f = f[f.chain == 'H']\n",
    "    f['seq_ids_list'] = f.index\n",
    "    f = f.groupby('sequence', as_index=False).agg(get_agg_dict(f))\n",
    "    f.index = f.seq_ids_list.str[0]\n",
    "    collapsed_ids = f[f.seq_ids_list.str.len() > 1].seq_ids_list\n",
    "    return f.drop(['seq_ids_list'], axis=1), collapsed_ids\n",
    "\n",
    "def collapse_pairs(f):\n",
    "    \"\"\"\n",
    "    Collapsing for identical heavy-light pair\n",
    "    \"\"\"\n",
    "    f = f[f.paired_seq.notna()]\n",
    "    f = f[f.germline_alignment.notna()]\n",
    "    f['seq_ids_list'] = f.index\n",
    "    agg_dict = get_agg_dict(f)\n",
    "    \n",
    "    cell_id_to_index = f.groupby('cell_id').agg({'chain' : list, 'sequence' : list})\n",
    "    cell_id_to_index['chain1'] = cell_id_to_index['chain'].str[0]\n",
    "    cell_id_to_index['chain2'] = cell_id_to_index['chain'].str[1]\n",
    "    H_mask = cell_id_to_index['chain1'] == 'H'\n",
    "    cell_id_to_index.loc[H_mask, 'joint_seq'] = cell_id_to_index.loc[H_mask, 'sequence'].str[0] + cell_id_to_index.loc[H_mask, 'sequence'].str[1]\n",
    "    L_mask = cell_id_to_index['chain1'] == 'L'\n",
    "    cell_id_to_index.loc[L_mask, 'joint_seq'] = cell_id_to_index.loc[L_mask, 'sequence'].str[1] + cell_id_to_index.loc[L_mask, 'sequence'].str[0]\n",
    "    f['joint_seq'] = f.cell_id.map(cell_id_to_index['joint_seq'])\n",
    "    f['joint_seq'] = f['joint_seq'] + f['chain']\n",
    "    f = f.groupby('joint_seq', as_index=False).agg(agg_dict)\n",
    "    f['old_seq_id'] = f.seq_ids_list.str[0]\n",
    "    f.index = f['old_seq_id']\n",
    "    \n",
    "    # Separating heavy and light frames\n",
    "    fh = f[f.chain == 'H']\n",
    "    collapsed_ids_h = fh[fh.seq_ids_list.str.len() > 1].seq_ids_list\n",
    "    fl = f[f.chain == 'L']\n",
    "    collapsed_ids_l = fl[fl.seq_ids_list.str.len() > 1].seq_ids_list\n",
    "    \n",
    "    # Rename indexes in a way that they are identical for the same pair\n",
    "    aux_ids = fh.old_seq_id.str.split('_')\n",
    "    fh.index = aux_ids.str[0] + '_' + aux_ids.str[1] + '_' + aux_ids.str[2] + '_' + aux_ids.str[3] + '_' + aux_ids.str[4]\n",
    "    aux_ids = fl.old_seq_id.str.split('_')\n",
    "    fl.index = aux_ids.str[0] + '_' + aux_ids.str[1] + '_' + aux_ids.str[2] + '_' + aux_ids.str[3] + '_' + aux_ids.str[4]\n",
    "    \n",
    "    return fh.drop(['seq_ids_list', 'joint_seq'], axis=1), fl.drop(['seq_ids_list', 'joint_seq'], axis=1), collapsed_ids_h, collapsed_ids_l\n",
    "\n",
    "\n",
    "def get_inverse_collapse(collapsed_ids):\n",
    "    \n",
    "    inverse_collapse = dict()\n",
    "    for new_id, ids in collapsed_ids.items():\n",
    "        for _id in ids:\n",
    "            inverse_collapse[_id] = new_id\n",
    "            \n",
    "    inv_collapse_fr = pd.DataFrame(index=inverse_collapse.keys())\n",
    "    inv_collapse_fr['new_id'] = inv_collapse_fr.index.map(inverse_collapse)\n",
    "    aux_ids = inv_collapse_fr.index.str.split('_').str\n",
    "    inv_collapse_fr['sample'] = aux_ids[2] + '_' + aux_ids[3] + '_' + aux_ids[4]\n",
    "    inv_collapse_fr['old_id'] = aux_ids[0] + '_' + aux_ids[1] + '_' + aux_ids[5] + '_' + aux_ids[6]\n",
    "    return inv_collapse_fr\n",
    "\n",
    "\n",
    "def map_inverse_collapse_heavy(pat, collapsed_ids, samp_frames):\n",
    "    # Remapping collapsed ids on the samples\n",
    "    inv_collapse = get_inverse_collapse(collapsed_ids)\n",
    "    for samp, row in metadata[metadata.patient == pat].iterrows():\n",
    "        id_map = inv_collapse[inv_collapse['sample'] == samp].groupby('old_id').agg({'new_id' : 'first'}).new_id\n",
    "        samp_frames[samp]['pat_heavy_id'] = samp_frames[samp].index.map(id_map)\n",
    "        # Mapping non collapsed ids\n",
    "        mask = samp_frames[samp].pat_heavy_id.isna()\n",
    "        aux_ids = samp_frames[samp][mask].index.str.split('_').str\n",
    "        samp_frames[samp].loc[mask, 'pat_heavy_id'] = aux_ids[0] + '_' + aux_ids[1] + '_' + samp + '_' + aux_ids[2] + '_' + aux_ids[3]\n",
    "    return samp_frames\n",
    "        \n",
    "def map_inverse_collapse_pairs(pat, collapsed_ids_h, collapsed_ids_l, samp_frames):\n",
    "    # Remapping collapsed ids on the samples\n",
    "    inv_collapse_h = get_inverse_collapse(ids_pairs_h)\n",
    "    inv_collapse_l = get_inverse_collapse(ids_pairs_l)\n",
    "    for samp, row in metadata[metadata.patient == pat].iterrows():\n",
    "        id_map_h = inv_collapse_h[inv_collapse_h['sample'] == samp].groupby('old_id').agg({'new_id' : 'first'}).new_id\n",
    "        id_map_l = inv_collapse_l[inv_collapse_l['sample'] == samp].groupby('old_id').agg({'new_id' : 'first'}).new_id\n",
    "        samp_frames[samp]['pat_pairs_id'] = samp_frames[samp].index.map(pd.concat((id_map_h,id_map_l)))\n",
    "        # Mapping non collapsed ids\n",
    "        mask = samp_frames[samp].pat_pairs_id.isna()\n",
    "        aux_ids = samp_frames[samp][mask].index.str.split('_').str\n",
    "        samp_frames[samp].loc[mask, 'pat_pairs_id'] = aux_ids[0] + '_' + aux_ids[1] + '_' + samp + '_' + aux_ids[2] + '_' + aux_ids[3]\n",
    "    return samp_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4284d473-9327-4a5e-a392-3287d9b8f66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv('metadata/metadata.tsv', sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71441dd3-c779-47f9-a405-c2d7d65638ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing\n",
      "pat2_t1_mc\n",
      "pat2_t2_mc\n",
      "pat2_t3_mc\n",
      "pat2_t1_pc\n",
      "pat2_t2_pc\n",
      "pat2_t3_pc\n",
      "pat2_t4_pc\n",
      "\n",
      "collapsing heavy\n",
      "editing the sample frames by adding the new id for the collapsed sequences\n",
      "hilary processing heavy\n",
      "\n",
      "collapsing pairs\n",
      "editing the sample frames by adding the new id for the collapsed sequences\n",
      "hilary processing pairs heavy\n",
      "hilary processing pairs light\n"
     ]
    }
   ],
   "source": [
    "pat = 2\n",
    "frame = pd.DataFrame()\n",
    "samp_frames = dict()\n",
    "\n",
    "print('importing')\n",
    "for samp, row in metadata[metadata.patient == pat].iterrows():\n",
    "    f = pd.read_csv('sequences/'+samp+'.tsv', sep='\\t', index_col=0, low_memory=False)\n",
    "    samp_frames[samp] = f.copy()\n",
    "    f.index = f.cell_id + '_' + samp + '_contig_' + f.index.str.split('_').str[-1]\n",
    "    frame = pd.concat((frame, f))\n",
    "    print(samp)\n",
    "print()\n",
    "\n",
    "print('collapsing heavy')\n",
    "frame_heavy, ids_heavy = collapse_heavy(frame.copy())\n",
    "print('editing the sample frames by adding the new id for the collapsed sequences')\n",
    "samp_frames = map_inverse_collapse_heavy(pat, ids_heavy, samp_frames)\n",
    "print('hilary processing heavy')\n",
    "#hilary_heavy_frame = hilary_preprocess(frame_heavy)\n",
    "print()\n",
    "\n",
    "print('collapsing pairs')\n",
    "frame_pairs_h, frame_pairs_l, ids_pairs_h, ids_pairs_l = collapse_pairs(frame.copy())\n",
    "print('editing the sample frames by adding the new id for the collapsed sequences')\n",
    "samp_frames = map_inverse_collapse_pairs(pat, ids_pairs_h, ids_pairs_l, samp_frames)\n",
    "print('hilary processing pairs heavy')\n",
    "#hilary_pairs_h_frame = hilary_preprocess(frame_pairs_h)\n",
    "#hilary_pairs_h_frame['old_seq_id'] = frame_pairs_h.old_seq_id\n",
    "print('hilary processing pairs light')\n",
    "#hilary_pairs_l_frame = hilary_preprocess(frame_pairs_l)\n",
    "#hilary_pairs_l_frame['old_seq_id'] = frame_pairs_l.old_seq_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef900eaf-5727-45ed-9be6-899b6d506a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_prefix = 'lineages/src_data/pat'+str(pat)+'_'\n",
    "#hilary_heavy_frame[:10000].to_csv(data_prefix + 'test.tsv', sep='\\t')\n",
    "#hilary_pairs_h_frame[:10000].to_csv(data_prefix + 'test_h.tsv', sep='\\t')\n",
    "#hilary_pairs_l_frame[:10000].to_csv(data_prefix + 'test_l.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ec22b23-7295-4a9a-9a22-6a2182ba81a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exporting hilary heavy\n",
      "exporting hilary pairs\n",
      "updating the sample frames with the collapsed ids\n"
     ]
    }
   ],
   "source": [
    "data_prefix = 'lineages/in_data/pat'+str(pat)+'_'\n",
    "\n",
    "print('exporting hilary heavy')\n",
    "#hilary_heavy_frame.to_csv(data_prefix + 'hilary_heavy.tsv', sep='\\t')\n",
    "\n",
    "print('exporting hilary pairs')\n",
    "#hilary_pairs_h_frame.to_csv(data_prefix + 'hilary_pairs_h.tsv', sep='\\t')\n",
    "# Aligning the indexes\n",
    "#hilary_pairs_l_frame = hilary_pairs_l_frame.loc[hilary_pairs_h_frame.index]\n",
    "#hilary_pairs_l_frame.to_csv(data_prefix + 'hilary_pairs_l.tsv', sep='\\t')\n",
    "\n",
    "print('updating the sample frames with the collapsed ids')\n",
    "for samp, row in metadata[metadata.patient == pat].iterrows():\n",
    "    samp_frames[samp].to_csv('sequences/'+samp+'.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec903945-19e2-4c2f-9e35-6ea42f36638f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
